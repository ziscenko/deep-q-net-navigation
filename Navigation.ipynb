{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "First, install the dependencies if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"E:/ML/Reinforcement/deep-reinforcement-learning/p1_navigation/Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement the agent and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 3e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 10        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an agent class\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        self.state_size = state_size,\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.Qnet_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.Qnet_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.Qnet_local.parameters(), lr=LR)\n",
    "\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.Qnet_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.Qnet_local(state)\n",
    "        self.Qnet_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.Qnet_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.Qnet_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.Qnet_local, self.Qnet_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=env_info.vector_observations[0].shape[0], \n",
    "              action_size=brain.vector_action_space_size, seed=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=3000\n",
    "max_t = 50000\n",
    "eps_start=1.0 \n",
    "eps_end=0.01 \n",
    "eps_decay=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Episode 1\tAverage Score: 0.002\n",
      "Episode 2\tAverage Score: 0.003\n",
      "Episode 3\tAverage Score: 0.334\n",
      "Episode 4\tAverage Score: 0.255\n",
      "Episode 5\tAverage Score: 0.606\n",
      "Episode 6\tAverage Score: 0.177\n",
      "Episode 7\tAverage Score: 0.148\n",
      "Episode 8\tAverage Score: 0.129\n",
      "Episode 9\tAverage Score: 0.0010\n",
      "Episode 10\tAverage Score: 0.1011\n",
      "Episode 11\tAverage Score: 0.0912\n",
      "Episode 12\tAverage Score: 0.0013\n",
      "Episode 13\tAverage Score: 0.0814\n",
      "Episode 14\tAverage Score: 0.0715\n",
      "Episode 15\tAverage Score: 0.1316\n",
      "Episode 16\tAverage Score: 0.1917\n",
      "Episode 17\tAverage Score: 0.1218\n",
      "Episode 18\tAverage Score: 0.2219\n",
      "Episode 19\tAverage Score: 0.2120\n",
      "Episode 20\tAverage Score: 0.3021\n",
      "Episode 21\tAverage Score: 0.2922\n",
      "Episode 22\tAverage Score: 0.3223\n",
      "Episode 23\tAverage Score: 0.3524\n",
      "Episode 24\tAverage Score: 0.2925\n",
      "Episode 25\tAverage Score: 0.2426\n",
      "Episode 26\tAverage Score: 0.3127\n",
      "Episode 27\tAverage Score: 0.3028\n",
      "Episode 28\tAverage Score: 0.2929\n",
      "Episode 29\tAverage Score: 0.3830\n",
      "Episode 30\tAverage Score: 0.4331\n",
      "Episode 31\tAverage Score: 0.3932\n",
      "Episode 32\tAverage Score: 0.3433\n",
      "Episode 33\tAverage Score: 0.4234\n",
      "Episode 34\tAverage Score: 0.4435\n",
      "Episode 35\tAverage Score: 0.6036\n",
      "Episode 36\tAverage Score: 0.6137\n",
      "Episode 37\tAverage Score: 0.5738\n",
      "Episode 38\tAverage Score: 0.6139\n",
      "Episode 39\tAverage Score: 0.5940\n",
      "Episode 40\tAverage Score: 0.6041\n",
      "Episode 41\tAverage Score: 0.6142\n",
      "Episode 42\tAverage Score: 0.6443\n",
      "Episode 43\tAverage Score: 0.7044\n",
      "Episode 44\tAverage Score: 0.7745\n",
      "Episode 45\tAverage Score: 0.7846\n",
      "Episode 46\tAverage Score: 0.7847\n",
      "Episode 47\tAverage Score: 0.7948\n",
      "Episode 48\tAverage Score: 0.8549\n",
      "Episode 49\tAverage Score: 0.8450\n",
      "Episode 50\tAverage Score: 0.9051\n",
      "Episode 51\tAverage Score: 0.9252\n",
      "Episode 52\tAverage Score: 0.9653\n",
      "Episode 53\tAverage Score: 0.9654\n",
      "Episode 54\tAverage Score: 0.9455\n",
      "Episode 55\tAverage Score: 0.9556\n",
      "Episode 56\tAverage Score: 0.8957\n",
      "Episode 57\tAverage Score: 0.8858\n",
      "Episode 58\tAverage Score: 0.8859\n",
      "Episode 59\tAverage Score: 0.9260\n",
      "Episode 60\tAverage Score: 0.8861\n",
      "Episode 61\tAverage Score: 0.9262\n",
      "Episode 62\tAverage Score: 0.9763\n",
      "Episode 63\tAverage Score: 1.0264\n",
      "Episode 64\tAverage Score: 1.0665\n",
      "Episode 65\tAverage Score: 1.0666\n",
      "Episode 66\tAverage Score: 1.0967\n",
      "Episode 67\tAverage Score: 1.0968\n",
      "Episode 68\tAverage Score: 1.1069\n",
      "Episode 69\tAverage Score: 1.1370\n",
      "Episode 70\tAverage Score: 1.1371\n",
      "Episode 71\tAverage Score: 1.2072\n",
      "Episode 72\tAverage Score: 1.2173\n",
      "Episode 73\tAverage Score: 1.3274\n",
      "Episode 74\tAverage Score: 1.3475\n",
      "Episode 75\tAverage Score: 1.3776\n",
      "Episode 76\tAverage Score: 1.3877\n",
      "Episode 77\tAverage Score: 1.3978\n",
      "Episode 78\tAverage Score: 1.4479\n",
      "Episode 79\tAverage Score: 1.4780\n",
      "Episode 80\tAverage Score: 1.5581\n",
      "Episode 81\tAverage Score: 1.5782\n",
      "Episode 82\tAverage Score: 1.6583\n",
      "Episode 83\tAverage Score: 1.6484\n",
      "Episode 84\tAverage Score: 1.6885\n",
      "Episode 85\tAverage Score: 1.6686\n",
      "Episode 86\tAverage Score: 1.7087\n",
      "Episode 87\tAverage Score: 1.7488\n",
      "Episode 88\tAverage Score: 1.8089\n",
      "Episode 89\tAverage Score: 1.8790\n",
      "Episode 90\tAverage Score: 1.9491\n",
      "Episode 91\tAverage Score: 1.9292\n",
      "Episode 92\tAverage Score: 1.9893\n",
      "Episode 93\tAverage Score: 1.9994\n",
      "Episode 94\tAverage Score: 2.0795\n",
      "Episode 95\tAverage Score: 2.0996\n",
      "Episode 96\tAverage Score: 2.1297\n",
      "Episode 97\tAverage Score: 2.1398\n",
      "Episode 98\tAverage Score: 2.1499\n",
      "Episode 99\tAverage Score: 2.17100\n",
      "Episode 100\tAverage Score: 2.21\n",
      "101\n",
      "Episode 101\tAverage Score: 2.24102\n",
      "Episode 102\tAverage Score: 2.34103\n",
      "Episode 103\tAverage Score: 2.43104\n",
      "Episode 104\tAverage Score: 2.48105\n",
      "Episode 105\tAverage Score: 2.53106\n",
      "Episode 106\tAverage Score: 2.67107\n",
      "Episode 107\tAverage Score: 2.75108\n",
      "Episode 108\tAverage Score: 2.81109\n",
      "Episode 109\tAverage Score: 2.85110\n",
      "Episode 110\tAverage Score: 2.91111\n",
      "Episode 111\tAverage Score: 2.93112\n",
      "Episode 112\tAverage Score: 2.99113\n",
      "Episode 113\tAverage Score: 3.00114\n",
      "Episode 114\tAverage Score: 3.04115\n",
      "Episode 115\tAverage Score: 3.12116\n",
      "Episode 116\tAverage Score: 3.23117\n",
      "Episode 117\tAverage Score: 3.29118\n",
      "Episode 118\tAverage Score: 3.33119\n",
      "Episode 119\tAverage Score: 3.35120\n",
      "Episode 120\tAverage Score: 3.39121\n",
      "Episode 121\tAverage Score: 3.43122\n",
      "Episode 122\tAverage Score: 3.47123\n",
      "Episode 123\tAverage Score: 3.49124\n",
      "Episode 124\tAverage Score: 3.57125\n",
      "Episode 125\tAverage Score: 3.69126\n",
      "Episode 126\tAverage Score: 3.72127\n",
      "Episode 127\tAverage Score: 3.82128\n",
      "Episode 128\tAverage Score: 3.89129\n",
      "Episode 129\tAverage Score: 3.94130\n",
      "Episode 130\tAverage Score: 3.97131\n",
      "Episode 131\tAverage Score: 4.06132\n",
      "Episode 132\tAverage Score: 4.15133\n",
      "Episode 133\tAverage Score: 4.19134\n",
      "Episode 134\tAverage Score: 4.22135\n",
      "Episode 135\tAverage Score: 4.29136\n",
      "Episode 136\tAverage Score: 4.39137\n",
      "Episode 137\tAverage Score: 4.51138\n",
      "Episode 138\tAverage Score: 4.54139\n",
      "Episode 139\tAverage Score: 4.66140\n",
      "Episode 140\tAverage Score: 4.71141\n",
      "Episode 141\tAverage Score: 4.72142\n",
      "Episode 142\tAverage Score: 4.82143\n",
      "Episode 143\tAverage Score: 4.90144\n",
      "Episode 144\tAverage Score: 4.89145\n",
      "Episode 145\tAverage Score: 4.98146\n",
      "Episode 146\tAverage Score: 5.07147\n",
      "Episode 147\tAverage Score: 5.15148\n",
      "Episode 148\tAverage Score: 5.19149\n",
      "Episode 149\tAverage Score: 5.29150\n",
      "Episode 150\tAverage Score: 5.38151\n",
      "Episode 151\tAverage Score: 5.50152\n",
      "Episode 152\tAverage Score: 5.63153\n",
      "Episode 153\tAverage Score: 5.66154\n",
      "Episode 154\tAverage Score: 5.70155\n",
      "Episode 155\tAverage Score: 5.84156\n",
      "Episode 156\tAverage Score: 5.95157\n",
      "Episode 157\tAverage Score: 6.05158\n",
      "Episode 158\tAverage Score: 6.14159\n",
      "Episode 159\tAverage Score: 6.18160\n",
      "Episode 160\tAverage Score: 6.30161\n",
      "Episode 161\tAverage Score: 6.36162\n",
      "Episode 162\tAverage Score: 6.37163\n",
      "Episode 163\tAverage Score: 6.47164\n",
      "Episode 164\tAverage Score: 6.54165\n",
      "Episode 165\tAverage Score: 6.60166\n",
      "Episode 166\tAverage Score: 6.65167\n",
      "Episode 167\tAverage Score: 6.74168\n",
      "Episode 168\tAverage Score: 6.81169\n",
      "Episode 169\tAverage Score: 6.87170\n",
      "Episode 170\tAverage Score: 6.98171\n",
      "Episode 171\tAverage Score: 6.98172\n",
      "Episode 172\tAverage Score: 7.01173\n",
      "Episode 173\tAverage Score: 7.01174\n",
      "Episode 174\tAverage Score: 7.09175\n",
      "Episode 175\tAverage Score: 7.19176\n",
      "Episode 176\tAverage Score: 7.25177\n",
      "Episode 177\tAverage Score: 7.31178\n",
      "Episode 178\tAverage Score: 7.36179\n",
      "Episode 179\tAverage Score: 7.45180\n",
      "Episode 180\tAverage Score: 7.46181\n",
      "Episode 181\tAverage Score: 7.51182\n",
      "Episode 182\tAverage Score: 7.55183\n",
      "Episode 183\tAverage Score: 7.66184\n",
      "Episode 184\tAverage Score: 7.75185\n",
      "Episode 185\tAverage Score: 7.86186\n",
      "Episode 186\tAverage Score: 7.95187\n",
      "Episode 187\tAverage Score: 8.04188\n",
      "Episode 188\tAverage Score: 8.09189\n",
      "Episode 189\tAverage Score: 8.09190\n",
      "Episode 190\tAverage Score: 8.08191\n",
      "Episode 191\tAverage Score: 8.22192\n",
      "Episode 192\tAverage Score: 8.23193\n",
      "Episode 193\tAverage Score: 8.26194\n",
      "Episode 194\tAverage Score: 8.28195\n",
      "Episode 195\tAverage Score: 8.33196\n",
      "Episode 196\tAverage Score: 8.35197\n",
      "Episode 197\tAverage Score: 8.40198\n",
      "Episode 198\tAverage Score: 8.43199\n",
      "Episode 199\tAverage Score: 8.43200\n",
      "Episode 200\tAverage Score: 8.48\n",
      "201\n",
      "Episode 201\tAverage Score: 8.49202\n",
      "Episode 202\tAverage Score: 8.49203\n",
      "Episode 203\tAverage Score: 8.48204\n",
      "Episode 204\tAverage Score: 8.58205\n",
      "Episode 205\tAverage Score: 8.65206\n",
      "Episode 206\tAverage Score: 8.70207\n",
      "Episode 207\tAverage Score: 8.66208\n",
      "Episode 208\tAverage Score: 8.66209\n",
      "Episode 209\tAverage Score: 8.64210\n",
      "Episode 210\tAverage Score: 8.68211\n",
      "Episode 211\tAverage Score: 8.77212\n",
      "Episode 212\tAverage Score: 8.81213\n",
      "Episode 213\tAverage Score: 8.90214\n",
      "Episode 214\tAverage Score: 9.00215\n",
      "Episode 215\tAverage Score: 8.98216\n",
      "Episode 216\tAverage Score: 9.02217\n",
      "Episode 217\tAverage Score: 9.02218\n",
      "Episode 218\tAverage Score: 9.05219\n",
      "Episode 219\tAverage Score: 9.13220\n",
      "Episode 220\tAverage Score: 9.19221\n",
      "Episode 221\tAverage Score: 9.23222\n",
      "Episode 222\tAverage Score: 9.31223\n",
      "Episode 223\tAverage Score: 9.42224\n",
      "Episode 224\tAverage Score: 9.42225\n",
      "Episode 225\tAverage Score: 9.41226\n",
      "Episode 226\tAverage Score: 9.49227\n",
      "Episode 227\tAverage Score: 9.51228\n",
      "Episode 228\tAverage Score: 9.53229\n",
      "Episode 229\tAverage Score: 9.55230\n",
      "Episode 230\tAverage Score: 9.61231\n",
      "Episode 231\tAverage Score: 9.66232\n",
      "Episode 232\tAverage Score: 9.67233\n",
      "Episode 233\tAverage Score: 9.69234\n",
      "Episode 234\tAverage Score: 9.81235\n",
      "Episode 235\tAverage Score: 9.78236\n",
      "Episode 236\tAverage Score: 9.78237\n",
      "Episode 237\tAverage Score: 9.73238\n",
      "Episode 238\tAverage Score: 9.82239\n",
      "Episode 239\tAverage Score: 9.79240\n",
      "Episode 240\tAverage Score: 9.80241\n",
      "Episode 241\tAverage Score: 9.86242\n",
      "Episode 242\tAverage Score: 9.74243\n",
      "Episode 243\tAverage Score: 9.74244\n",
      "Episode 244\tAverage Score: 9.80245\n",
      "Episode 245\tAverage Score: 9.72246\n",
      "Episode 246\tAverage Score: 9.75247\n",
      "Episode 247\tAverage Score: 9.78248\n",
      "Episode 248\tAverage Score: 9.85249\n",
      "Episode 249\tAverage Score: 9.87250\n",
      "Episode 250\tAverage Score: 9.78251\n",
      "Episode 251\tAverage Score: 9.81252\n",
      "Episode 252\tAverage Score: 9.70253\n",
      "Episode 253\tAverage Score: 9.84254\n",
      "Episode 254\tAverage Score: 9.92255\n",
      "Episode 255\tAverage Score: 9.79256\n",
      "Episode 256\tAverage Score: 9.83257\n",
      "Episode 257\tAverage Score: 9.75258\n",
      "Episode 258\tAverage Score: 9.73259\n",
      "Episode 259\tAverage Score: 9.74260\n",
      "Episode 260\tAverage Score: 9.74261\n",
      "Episode 261\tAverage Score: 9.82262\n",
      "Episode 262\tAverage Score: 9.92263\n",
      "Episode 263\tAverage Score: 9.94264\n",
      "Episode 264\tAverage Score: 9.97265\n",
      "Episode 265\tAverage Score: 9.96266\n",
      "Episode 266\tAverage Score: 10.01267\n",
      "Episode 267\tAverage Score: 10.07268\n",
      "Episode 268\tAverage Score: 10.04269\n",
      "Episode 269\tAverage Score: 10.03270\n",
      "Episode 270\tAverage Score: 10.03271\n",
      "Episode 271\tAverage Score: 10.12272\n",
      "Episode 272\tAverage Score: 10.26273\n",
      "Episode 273\tAverage Score: 10.31274\n",
      "Episode 274\tAverage Score: 10.36275\n",
      "Episode 275\tAverage Score: 10.39276\n",
      "Episode 276\tAverage Score: 10.40277\n",
      "Episode 277\tAverage Score: 10.39278\n",
      "Episode 278\tAverage Score: 10.42279\n",
      "Episode 279\tAverage Score: 10.40280\n",
      "Episode 280\tAverage Score: 10.48281\n",
      "Episode 281\tAverage Score: 10.56282\n",
      "Episode 282\tAverage Score: 10.61283\n",
      "Episode 283\tAverage Score: 10.63284\n",
      "Episode 284\tAverage Score: 10.63285\n",
      "Episode 285\tAverage Score: 10.63286\n",
      "Episode 286\tAverage Score: 10.57287\n",
      "Episode 287\tAverage Score: 10.51288\n",
      "Episode 288\tAverage Score: 10.51289\n",
      "Episode 289\tAverage Score: 10.58290\n",
      "Episode 290\tAverage Score: 10.69291\n",
      "Episode 291\tAverage Score: 10.61292\n",
      "Episode 292\tAverage Score: 10.67293\n",
      "Episode 293\tAverage Score: 10.76294\n",
      "Episode 294\tAverage Score: 10.82295\n",
      "Episode 295\tAverage Score: 10.91296\n",
      "Episode 296\tAverage Score: 10.97297\n",
      "Episode 297\tAverage Score: 11.04298\n",
      "Episode 298\tAverage Score: 11.08299\n",
      "Episode 299\tAverage Score: 11.04300\n",
      "Episode 300\tAverage Score: 11.04\n",
      "301\n",
      "Episode 301\tAverage Score: 11.09302\n",
      "Episode 302\tAverage Score: 11.09303\n",
      "Episode 303\tAverage Score: 11.10304\n",
      "Episode 304\tAverage Score: 11.09305\n",
      "Episode 305\tAverage Score: 11.11306\n",
      "Episode 306\tAverage Score: 11.09307\n",
      "Episode 307\tAverage Score: 11.17308\n",
      "Episode 308\tAverage Score: 11.30309\n",
      "Episode 309\tAverage Score: 11.38310\n",
      "Episode 310\tAverage Score: 11.31311\n",
      "Episode 311\tAverage Score: 11.20312\n",
      "Episode 312\tAverage Score: 11.22313\n",
      "Episode 313\tAverage Score: 11.27314\n",
      "Episode 314\tAverage Score: 11.31315\n",
      "Episode 315\tAverage Score: 11.36316\n",
      "Episode 316\tAverage Score: 11.34317\n",
      "Episode 317\tAverage Score: 11.46318\n",
      "Episode 318\tAverage Score: 11.53319\n",
      "Episode 319\tAverage Score: 11.59320\n",
      "Episode 320\tAverage Score: 11.60321\n",
      "Episode 321\tAverage Score: 11.70322\n",
      "Episode 322\tAverage Score: 11.67323\n",
      "Episode 323\tAverage Score: 11.66324\n",
      "Episode 324\tAverage Score: 11.76325\n",
      "Episode 325\tAverage Score: 11.80326\n",
      "Episode 326\tAverage Score: 11.80327\n",
      "Episode 327\tAverage Score: 11.75328\n",
      "Episode 328\tAverage Score: 11.79329\n",
      "Episode 329\tAverage Score: 11.84330\n",
      "Episode 330\tAverage Score: 11.89331\n",
      "Episode 331\tAverage Score: 11.90332\n",
      "Episode 332\tAverage Score: 11.97333\n",
      "Episode 333\tAverage Score: 12.02334\n",
      "Episode 334\tAverage Score: 12.01335\n",
      "Episode 335\tAverage Score: 12.08336\n",
      "Episode 336\tAverage Score: 12.04337\n",
      "Episode 337\tAverage Score: 12.11338\n",
      "Episode 338\tAverage Score: 12.10339\n",
      "Episode 339\tAverage Score: 12.15340\n",
      "Episode 340\tAverage Score: 12.20341\n",
      "Episode 341\tAverage Score: 12.24342\n",
      "Episode 342\tAverage Score: 12.40343\n",
      "Episode 343\tAverage Score: 12.37344\n",
      "Episode 344\tAverage Score: 12.38345\n",
      "Episode 345\tAverage Score: 12.53346\n",
      "Episode 346\tAverage Score: 12.47347\n",
      "Episode 347\tAverage Score: 12.47348\n",
      "Episode 348\tAverage Score: 12.38349\n",
      "Episode 349\tAverage Score: 12.36350\n",
      "Episode 350\tAverage Score: 12.40351\n",
      "Episode 351\tAverage Score: 12.27352\n",
      "Episode 352\tAverage Score: 12.23353\n",
      "Episode 353\tAverage Score: 12.18354\n",
      "Episode 354\tAverage Score: 12.17355\n",
      "Episode 355\tAverage Score: 12.25356\n",
      "Episode 356\tAverage Score: 12.21357\n",
      "Episode 357\tAverage Score: 12.24358\n",
      "Episode 358\tAverage Score: 12.32359\n",
      "Episode 359\tAverage Score: 12.45360\n",
      "Episode 360\tAverage Score: 12.46361\n",
      "Episode 361\tAverage Score: 12.43362\n",
      "Episode 362\tAverage Score: 12.50363\n",
      "Episode 363\tAverage Score: 12.46364\n",
      "Episode 364\tAverage Score: 12.47365\n",
      "Episode 365\tAverage Score: 12.54366\n",
      "Episode 366\tAverage Score: 12.58367\n",
      "Episode 367\tAverage Score: 12.57368\n",
      "Episode 368\tAverage Score: 12.62369\n",
      "Episode 369\tAverage Score: 12.67370\n",
      "Episode 370\tAverage Score: 12.74371\n",
      "Episode 371\tAverage Score: 12.77372\n",
      "Episode 372\tAverage Score: 12.73373\n",
      "Episode 373\tAverage Score: 12.72374\n",
      "Episode 374\tAverage Score: 12.72375\n",
      "Episode 375\tAverage Score: 12.65376\n",
      "Episode 376\tAverage Score: 12.69377\n",
      "Episode 377\tAverage Score: 12.78378\n",
      "Episode 378\tAverage Score: 12.76379\n",
      "Episode 379\tAverage Score: 12.82380\n",
      "Episode 380\tAverage Score: 12.78381\n",
      "Episode 381\tAverage Score: 12.79382\n",
      "Episode 382\tAverage Score: 12.79383\n",
      "Episode 383\tAverage Score: 12.74384\n",
      "Episode 384\tAverage Score: 12.69385\n",
      "Episode 385\tAverage Score: 12.74386\n",
      "Episode 386\tAverage Score: 12.84387\n",
      "Episode 387\tAverage Score: 12.90388\n",
      "Episode 388\tAverage Score: 12.94389\n",
      "Episode 389\tAverage Score: 12.89390\n",
      "Episode 390\tAverage Score: 12.91391\n",
      "Episode 391\tAverage Score: 12.89392\n",
      "Episode 392\tAverage Score: 12.82393\n",
      "Episode 393\tAverage Score: 12.85394\n",
      "Episode 394\tAverage Score: 12.81395\n",
      "Episode 395\tAverage Score: 12.70396\n",
      "Episode 396\tAverage Score: 12.74397\n",
      "Episode 397\tAverage Score: 12.78398\n",
      "Episode 398\tAverage Score: 12.78399\n",
      "Episode 399\tAverage Score: 12.89400\n",
      "Episode 400\tAverage Score: 12.96\n",
      "401\n",
      "Episode 401\tAverage Score: 13.06\n",
      "Environment solved in 301 episodes!\tAverage Score: 13.06\n"
     ]
    }
   ],
   "source": [
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "eps = eps_start                    # initialize epsilon\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0] \n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, eps)\n",
    "        result = env.step(int(action))[brain_name] \n",
    "        next_state = result.vector_observations[0]   # get the next state\n",
    "        reward = result.rewards[0]                   # get the reward\n",
    "        done = result.local_done[0]                  # see if episode has finished\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    if np.mean(scores_window)>=13.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        torch.save(agent.Qnet_local.state_dict(), 'checkpoint.pth')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU1fnH8c+TkBUSAiQsYQ37bghhx8iigLRFEC3gVkFBq7hU695WbGu12tbqr25RFEWICLijyCYiOwHCvgYSCAQIZCEhe3J+f8wQQwgQIDN3MvO8Xy9emblzZ+53TjIPd8699xwxxqCUUspzeFkdQCmllHNp4VdKKQ+jhV8ppTyMFn6llPIwWviVUsrD1LI6QFWEhoaaVq1aWR1DKaVqlI0bN540xoRVXF4jCn+rVq2Ij4+3OoZSStUoIpJc2XLt6lFKKQ+jhV8ppTyMFn6llPIwNaKPvzJFRUWkpKSQn59vdRTlJvz9/WnWrBk+Pj5WR1HKoWps4U9JSSEoKIhWrVohIlbHUTWcMYZTp06RkpJCRESE1XGUcqga29WTn59PgwYNtOiraiEiNGjQQL9BKo9QYws/oEVfVSv9e1KeokYXfqWUqomKSkpJTMu56Dr5RSVM+3oH6WcKq337Wviv0hdffIGIsHv3bqujXFJCQgLffffdZT/v6NGj3HLLLZdcb+TIkWRmZl5JNKU8xun8IsbHrmXov39iyL+W89S8rZzMKThvvWlf72DG6iS2Hcmq9gxa+K9SXFwcAwcO5NNPP62W1yspKamW16nMxQp/cXHxBZ8XHh7OvHnzLvn63333HSEhIVecz1Eu9t6UcpT8ohIWbE0lv+iXz7QxhsfmJLAxOYM6frXILSzhi4QjRP99CRNi15KSkQvArtTTfLrhMPfFtOa69ueNuHDVtPBfhZycHFatWsX06dPPKfzjxo07p8DefffdzJ8/n5KSEp544gl69epF9+7deffddwFYvnw5gwcP5rbbbqNbt24AjB49mp49e9KlSxdiY2PLXmv69Om0b9+eQYMGMXnyZKZOnQpAWloaY8eOpVevXvTq1YtVq1adk7WwsJC//OUvzJkzh8jISObMmcO0adOYMmUKw4YN46677iIpKYlrr72WqKgooqKiWL16NQBJSUl07doVgBkzZnDzzTczYsQI2rVrx5NPPlm2jVatWnHy5EmSkpLo1KkTkydPpkuXLgwbNoy8vDwANmzYQPfu3enXrx9PPPFE2euWl5qaSkxMDJGRkXTt2pWff/4ZgIULFxIVFcU111zD0KFDAUhPT2f06NF0796dvn37snXrVoDz3tuF2l6pq/XT3jSmfb2DU+X22l/6fhfdX1jEg7M30fHPC+nwp+/558LdrE48xZJdJ3h2ZEe2vzCctc8O5aUxts/8mgOneChuMx+sPMiNr/9MoK83Dwxq65DMNfZ0zvJe+GYHO4+ertbX7BwezPO/6XLRdb788ktGjBhB+/btqV+/Pps2bSIqKorx48czZ84cRo4cSWFhIUuXLuXtt99m+vTp1K1blw0bNlBQUMCAAQMYNmwYAOvXr2f79u1lpxJ+8MEH1K9fn7y8PHr16sXYsWMpKCjgb3/7G5s2bSIoKIghQ4ZwzTXXAPDII4/whz/8gYEDB3Lo0CGGDx/Orl27yrL6+vry17/+lfj4eP73v/8BtuK4ceNGVq5cSUBAALm5uSxevBh/f3/27dvHhAkTKh0jKSEhgc2bN+Pn50eHDh146KGHaN68+Tnr7Nu3j7i4ON577z1++9vfMn/+fO644w4mTpxIbGws/fv35+mnn660XWfPns3w4cN57rnnKCkpITc3l7S0NCZPnsyKFSuIiIggPT0dgOeff54ePXrw5ZdfsmzZMu666y4SEhIAznlvsbGxlba9nrqprsaiHceYMnMjALPXHaJhsB8juzUhdsUBAPq3aUCzegGkZRfw9vJEZq5JpnGwP3f1a1X2GmN7NmN0j6bMXpfMn7/aweZDmXRtGszv+rWibqBjrilxi8Jvlbi4OB599FEAxo8fT1xcHFFRUdx44408/PDDFBQUsHDhQmJiYggICGDRokVs3bq1rNskKyuLffv24evrS+/evc8pQm+88QZffPEFAIcPH2bfvn0cO3aM6667jvr16wNw6623snfvXgCWLFnCzp07y55/+vRpsrOzCQoKuuh7GDVqFAEBAYDtoripU6eSkJCAt7d32WtXNHToUOrWrQtA586dSU5OPq/wR0REEBkZCUDPnj1JSkoiMzOT7Oxs+vfvD8Btt93Gt99+e97r9+rVi0mTJlFUVMTo0aOJjIxk+fLlxMTElLXR2TZYuXIl8+fPB2DIkCGcOnWKrKys897bhdpeC7+6UttSsvjj3C10bRrMi6O78f32Y3y+KYXYFQdoVi+AxX+4jgBfb8DWxfP3Bbv4YvMR3rojCn8f73Ney9tLuKVnc77ffoyB7UL5/XVtHHqWmVsU/kvtmTvCqVOnWLZsGdu3b0dEKCkpQUR45ZVX8Pf3Z9CgQfzwww/MmTOHCRMmALZf/v/93/8xfPjwc15r+fLl1K5d+5z7S5YsYc2aNQQGBjJo0CDy8/MxxlwwT2lpKWvWrCkrdFVVfruvvfYajRo1YsuWLZSWluLv71/pc/z8/Mpue3t7V9qHXnGdvLy8i+YvLyYmhhUrVrBgwQLuvPNOnnjiCUJCQir9IFT2mmfXK//eLtT2Sl2u7Uey+M/ivazYm0bjuv68c0dPmtUL5JrmIdzepwUfrDrIfTFtyoo+2P4m//zrzjw3shNeXpUX9ABfb2ZP7uuU96B9/Fdo3rx53HXXXSQnJ5OUlMThw4eJiIhg5cqVgO0bwIcffsjPP/9cVmyGDx/O22+/TVFREQB79+7lzJkz5712VlYW9erVIzAwkN27d7N27VoAevfuzU8//URGRgbFxcVle7oAw4YNK+vCAcq6O8oLCgoiOzv7gu8pKyuLJk2a4OXlxcyZM6v9QHO9evUICgoqez8XOiCenJxMw4YNmTx5Mvfccw+bNm2iX79+/PTTTxw8eBCgrKsnJiaGWbNmAbb/MENDQwkODj7vNava9sr97Tx6mphXfmTh9mOUllZtZ+R0fhHv/3yAjckZjHt3DSv3n6Rny3rETe5Ls3qBZes1rx/I87/pQuO6le80XajoO5tb7PFbIS4u7rw+6rFjxzJ79myuvfbasoOKo0aNwtfXF4B7772XpKQkoqKiMMYQFhbGl19+ed5rjxgxgnfeeYfu3bvToUMH+va17QU0bdqUZ599lj59+hAeHk7nzp3LulzeeOMNHnzwQbp3705xcTExMTG8884757zu4MGDefnll4mMjOSZZ545b7sPPPAAY8eOZe7cuQwePPicPebqMn36dCZPnkzt2rUZNGhQWf7yli9fzquvvoqPjw916tTh448/JiwsjNjYWG6++WZKS0tp2LAhixcvZtq0aUycOJHu3bsTGBjIRx99VOl2q9r2yv3FrT/EofRc7v9kI9e1D2PGxF4X7FZZtvs4f/1mJ0mncsuW1Q3wYdFj19I05PK+XbsSqerXbytFR0ebigcZd+3aRadOnSxKZJ2cnBzq1KlDcXExY8aMYdKkSYwZM8bqWFV2Nj/Ayy+/TGpqKq+//rrFqX7hqX9XnuDdnxL5eE0yRzLz6NWqHo3rBvDNlqN0aBREQXEJ3zw0kCD/Xw6mGmO44bUVJJ86Qx2/WlzXPoxWobUZ3qUxnZqc/63SFYnIRmNMdMXlusdfw0ybNo0lS5aQn5/PsGHDGD16tNWRLsuCBQt46aWXKC4upmXLlsyYMcPqSMoDGGPKin7bhnX406860yU8mMLiEn7YcRyAb7emMqF3i7LnbEjKYP+JHF6+uRvjejV3qyE9tPDXMP/617+sjnBVxo0bx7hx46yOoTzMztTTHMnM42+ju3Jn35Zly9+9M5qSUsONr6/gzR/30yeiPrtSs1m2+wSbD2XQMMiP31wT7lZFH2p44TfGuN0vRFmnJnR7qsu3/0QO934UT7B/LUZ0aXze495ewks3d2fyx/Hc+PrPFBSXAuAltv8YavvV6DJZKYe9IxH5APg1cMIY09W+7FXgN0AhkAhMNMZc0eAu/v7+nDp1SodmVtXi7Hj8FzqFVdUseYUlpGUX8M6KRL7ZchS/Wt7Mua8fYUF+la7fs2U9PruvL498msDJnAL+/OvOdG4STOuwOk5O7hwOO7grIjFADvBxucI/DFhmjCkWkX8CGGOeutRrVXZwV2fgUtVNZ+Cq+UpKDRuS0nlg1qayUS0jQmvzwd29iAit/rPUXJ3TD+4aY1aISKsKyxaVu7sWuPSQjxfg4+OjV10qpcoUFpcyZWY8y/ekASAC797Rkxs6N9JegQqs7LyaBMy50IMiMgWYAtCiRYsLraaUUpzKKeDF73axfE8ad/ZtyegeTenZsp7VsVyWJYVfRJ4DioFZF1rHGBMLxIKtq8dJ0ZRSNUxiWg4TYtdyIruAu/u3Ytoo5w/hUtM4vfCLyO+wHfQdavQ0CqXUZSopNby2eC/fbD1K05AA9h7PAQyfTulLn4j6VserEZxa+EVkBPAUcJ0xJvdS6yul1FkncwpIyy5g3YFT/O/H/bQJq83qxFOE1vEjbnJf2jW6+Ei06heOPJ0zDhgEhIpICvA88AzgByy2H2xZa4y531EZlFLuwRjD/TM3Ep+cAUDf1vWJm9yXFftO0jU8mAZ1Kj9NU1XOkWf1TKhk8XRHbU8p5Z6y84v449wtxCdn0CjYjw6Ng3liWAdExCHTEnoC97skTSnlNowxPDF3Kz/sOM6tPZvx9zFd8avlfeknqovSwq+UclkzViexcMcxnh3ZkSkxbayO4zZ0IhallOUSDmey/8S5kwQdy8rnzR/3M7BtKJOvbW1RMveke/xKKUulZOQy+s1VAKx/bigNg/x5a/l+Xlm4B4DJMa31yttqpoVfKWWp/y7Zd87tIP9avPvTATo3CWZIx4Zc2zbUwnTuSQu/Usoy2flFLNiayoTezQGYve4QAHf2bckLo7q4zBy17kYLv1LKaUpLDWcKizmZU0j6mULi1h8ir6iE30Y3J7SOH6lZ+dzQuRG39W6h3TsOpIVfKeVQ249k8eevtnN9p0bMjT98zsTlACO7NaZHC9uAajMm9rYiosfRwq+UcpjD6blMnLGBtOwCNh/KJMivFg8PbUfTEH98a3lRUFTKiK7nz4qlHEsLv1LKITLOFPK7D9dTUFTCdw9fS05BMS0bBNIoWGc5s5oWfqWUQ0z7Zgcp6Xl8cm8fOocHWx1HlaMXcCmlqt3qxJN8lXCUyTER9Nahkl2OFn6lVLVKP1PIg7M20SasNr8f1NbqOKoS2tWjlLoipaW2eZTOnmufcDiTF77ZQd0AH7Lyivh0Sj/q+GmJcUX6W1FKXZbViSf585fbSUw7g18tL/45tjujezTl79/uZPOhTADu7t+KDo11YhRXpYVfKVVlu1JPM/HDDTQNCeDhoe1YuS+NJ+dtZdOhDOKTM+jaNJhuTevy7MhOVkdVF6GFXylVJSWlhqc/30aQfy0+u78foXX8mDSgFWPeWs3Ha5Lp0CiIeff3x99Hx8t3dVr4lVIXtP1IFqsTT3Isq4DP4g+TU1DM6+MjCbVPdRgS6Mvnv+/PuoPp9GvdQIt+DaGFXyl1HmMMry3eyxvL9pctq+NXixFdGjPqmvBz1q1X21evvq1htPArpc7zyg97eHt5Irf2bMZTN3bEGAgL0gnN3YUWfqXUObYfyeLt5YmM79Wcf4zppkMjuyGHXcAlIh+IyAkR2V5uWX0RWSwi++w/6zlq+0qpK/Pl5iP4eAtP39hRi76bcuSVuzOAERWWPQ0sNca0A5ba7yulLLZ013F2HM0iO7+IzzcfYXCHhoQE+lodSzmIw7p6jDErRKRVhcU3AYPstz8ClgNPOSqDUurS4tYf4pnPt52z7KEh7SxKo5zB2X38jYwxqQDGmFQRaXihFUVkCjAFoEWLFk6Kp5R7OZ1fhF8tL/xqeZ+3/JO1ycxYlcSJ7AJ6tqzHde3DOJCWQ5/WDejWrK5FiZUzuOzBXWNMLBALEB0dbSyOo5RLyThTSC1vIcjfp2yZMYYNSRkE+nrTtWldTucX0fNvixnWpTFv3hZ1znp3vr+OLSlZ9GgRwshuTXhyRAcCfV22HKhq5uzf9HERaWLf228CnHDy9pWqUUpKDfM3pdAnoj6Ngv15Y+k+GgX78/zXO4hqEcJHk3qTkpFHpybB/HvRXv73o+28+ydHdCAlI4+iEsOCramkZa/hrdujCK3jx6Kdx9mSksWLY7pye5+WFr9DZQUxxnE70/Y+/m+NMV3t918FThljXhaRp4H6xpgnL/U60dHRJj4+3mE5lXJVC7am8uDsTZdc77fRzZi7MYWu4XXZdiSrbHnrsNocSDsDwOAOYQzp2JB/LdpLgzq+LHo0hlreOjK7OxORjcaY6IrLHXk6ZxywBuggIikicg/wMnCDiOwDbrDfV0pVUFJqWLX/JG/+uB8ReHhIW7wE+rVuQNzkvix7/DqC/X/5wj53YwrjezXns/v68fGkXyYsf+bGTiz6Qwx/HNae5XvT+PNXOygsLuWlMd206Hswh+7xVxfd41eepKTU8OCsTSzccQx/H9uwxzdFNiX9TCGBvt5l4+HkF5VQagypWfkUFpfSqckv0xseSMvhk7WHeGZkR3zsBf746XwKikoJC/IjwFfH1PEEF9rj18KvlAs5fjqff36/m883H+HR69txz8CIcw7gKnU5LlT49TC+Ui4g/Uwhz36+jcW7jlNSanhoSFsevb691bGUm9LCr5SFdh49zVcJR/h2aypp2QXce20EE3q1oFVobaujKTemhV8pixhjeOaLbWw5bJuu8LEb2vPwUL1iVjmeFn6lLLI68RRbDmfyt9FdGR0ZrhOTK6fRvzSlLJBxppA/f7WdpiEB3Nqzmc5cpZxKC79STpaZW8jYt1eTkpHHB3f30qKvnE4Lv1JO9uS8raRk5DHznt70ad3A6jjKA2nhV8rBjDFkFxSz4WA6P+45waKdx3nmxo5a9JVltPAr5WBvLU/k1R/2lN0fG9WMSQMjLEykPJ0WfqUc6FROAW/ZR8z80686Mb53Cz17R1lO/wKVcqC3lieSV1TCksdiaNswyOo4SgGOnXNXKY82Z8MhZqxOYmxUMy36yqVo4Veqmmw5nMmsdcmUlhqOZubxl6920K91A54f1cXqaEqdQ7t6lKoGW1MyGf3WKoyBF77eSYkx1PISXh7bTfv0lcvRv0ilrlJ+UQkPxW0mvG4AEwe04uDJM+w/kcMjQ9vRrF6g1fGUOo8WfqWuUGJaDjuOnubrhKMkn8rlk3v6MLBdqNWxlLokLfxKXaZNhzL4YtMRZq5NLlvWr3UDLfqqxtDCr1QVlZYaXl+6j9eX7gPgxq6NefT69mTnF9EmrI7F6ZSqOi38SlVBalYed3+wgT3Hsxkb1YxnRnYktI6f1bGUuiJa+JW6iANpOczZcJiP1yTj7SW8Nu4abrqmKV5eYnU0pa6YFn6lLuD46XxGvvEz+UWljI4MZ+qQtnohlnILlhR+EfkDcC9ggG3ARGNMvhVZlKqotNSQkJLJR6uTyC8qZfbkPvRvowdulftweuEXkabAw0BnY0yeiHwGjAdmODuLUhWVlhr+8FkCXyUcBeCaZnW16Cu3Y1VXTy0gQESKgEDgqEU5lDrHN1uP8lXCUSYNiKBXq3p0ahJsdSSlqp3TC78x5oiI/As4BOQBi4wxiyquJyJTgCkALVq0cG5I5ZHyi0p4ZeEeuoQH86dfddIDuMptOX2QNhGpB9wERADhQG0RuaPiesaYWGNMtDEmOiwszNkxlQf6aHUSRzLzeG6kFn3l3qwYnfN64KAxJs0YUwR8DvS3IIdSZfKLSnjv54Nc2y6U/m21T1+5NysK/yGgr4gEiogAQ4FdFuRQqszc+MOczCng99e1sTqKUg7n9MJvjFkHzAM2YTuV0wuIdXYOpc7KKSjmv0v20TuiPv3a6AToyv1ZclaPMeZ54Hkrtq1URZ+uP8SpM4W8d2NHbF9ClXJvOgOX8mj7T2Tzzk8H6B1Rn6gW9ayOo5RT6JANyiMVFpfywaqDvL08ER9vL/56k06PqDyH7vErj/T+ygO8/P1uWtQP5IsH+tOxsV6opTyH7vErj7Pz6GneXLaf6zs15P3f9bI6jlJOp3v8yqMcycxj4oz1BPn78LfRXa2Oo5QltPArj5GdX8TED9eTW1DCjEm9aFI3wOpISlmiyoVfRAaKyET77TARiXBcLKWqX+yKA+w9nsO7d/bUPn3l0apU+EXkeeAp4Bn7Ih/gE0eFUqq6ncjO58NVSYzs1liHZFAer6oHd8cAPbBdbYsx5qiI6FREyuVtSEpnxd40ZqxOorC4lMeHdbA6klKWq2rhLzTGGBExACJS24GZlKoWmw9lcOs7awCIbB7CxAGtaBNWx+JUSlmvqoX/MxF5FwgRkcnAJOA9x8VS6uq9vTwRX28vvpo6QCdUUaqcKhV+Y8y/ROQG4DTQAfiLMWaxQ5MpdRXmxh9m0c7jPHZDey36SlVwycIvIt7AD8aY6wEt9srl7TmWzZ++3E6/1g14YJAOs6xURZc8q8cYUwLkikhdJ+RR6qoYY3jhmx0E+nrzxoQe1PLWS1WUqqiqffz5wDYRWQycObvQGPOwQ1IpdYVeW7KP1YmneGFUF8KC/KyOo5RLqmrhX2D/p5TL+mbLUd5Yuo/fRjfjzr4trY6jlMuq6sHdj0TEF2hvX7THPl+uUi4hMS2Hp+ZvpWfLerw4pptOlq7URVSp8IvIIOAjIAkQoLmI/M4Ys8Jx0ZSqupe+200tL+HN26Lw0X59pS6qql09/waGGWP2AIhIeyAO6OmoYEpVxen8IiZ/FM+6g+k8fkN7Gtf1tzqSUi6vqrtGPmeLPoAxZi+28XqUstRzX2wnPjmDqYPbMjmmtdVxlKoRqrrHHy8i04GZ9vu3AxsdE0mpqknJyGXB1qNMiWnDH4frGDxKVVVV9/h/D+wAHgYeAXYC91/pRkUkRETmichuEdklIv2u9LWUZzpTUMyT87YCcGc/PYNHqctR1T3+WsDrxpj/QNnVvFdzkvTrwEJjzC32s4UCr+K1lAd6Y9k+1hw4xStju9M0RCdUUepyVHWPfylQ/tMVACy5kg2KSDAQA0wHMMYUGmMyr+S1lGc6lmUbW39Mj6bcGt3c6jhK1ThVLfz+xpics3fst690L701kAZ8KCKbReT9yoZ5FpEpIhIvIvFpaWlXuCnljt7/+QAlpYZHh7a/9MpKqfNUtfCfEZGos3dEJBrIu8Jt1gKigLeNMT2wDQHxdMWVjDGxxphoY0x0WFjYFW5KuZuMM4XMWneIUdeE06KB9hAqdSWq2sf/KDBXRI4CBggHxl3hNlOAFGPMOvv9eVRS+JWqzMdrkskrKtFRN5W6Chfd4xeRXiLS2BizAegIzAGKgYXAwSvZoDHmGHBYRM6efzcU21lCSl2UMYZ5mw4zsG0o7RrpzJ9KXalLdfW8CxTab/cDngXeBDKA2KvY7kPALBHZCkQC/7iK11IeYsG2VA6n5zGmR1OroyhVo12qq8fbGJNuvz0OiDXGzAfmi0jClW7UGJMARF/p85Xneen7Xbz70wG6hAdzY7fGVsdRqka71B6/t4ic/c9hKLCs3GNVPT6g1FVJycgldsUBbooMZ979/Qn01T89pa7GpT5BccBPInIS21k8PwOISFsgy8HZlAJgbnwKAE8M70CAr7fFaZSq+S5a+I0xL4rIUqAJsMgYY+wPeWHrp1fKoXILi5m5NplB7cNoVk9P31SqOlzyO7MxZm0ly/Y6Jo5S55q97hDpZwqZOqSd1VGUchs6Y4VyWflFJcSuOED/Ng3o2bKe1XGUchta+JXLmht/mBPZBUwd0tbqKEq5FS38yiVtP5LF60v3Ed2yHv1aN7A6jlJuRQu/cjn5RSXc/8lGfLy9eHFMN0R04nSlqpOeEK1czvSVB0nJyGP2vX3o0FiHZlCquukev3Ipx7LyefPH/Yzo0pj+bUOtjqOUW9LCr1zGoh3H+M3/VlJcanh2ZCer4yjltrSrR7mEpJNnmDp7M4UlpTwytJ2Ota+UA2nhVy7hlR924+MtLH18MM3q6Ry6SjmSdvUoS50pKOaxzxL4btsxJg2MoHn9QD2LRykH0z1+ZalHPk1g6e7jjI4MZ3JMa6vjKOURtPAry+w7ns2SXcd57Ib2PDxUx+JRylm0q0dZ4lhWPg/FbcbX24vb+7SwOo5SHkULv7LEOz8lciDtDG/dHkWDOn5Wx1HKo2jhV06XU1DM/I0pjOzWmOs7N7I6jlIeRwu/crqPVieRXVDMxAERVkdRyiNp4VdOdTg9l3eWJzKkY0OuaR5idRylPJIWfuU0y3YfZ8xbqwB4YVQXi9Mo5bksK/wi4i0im0XkW6syKOfZdCiDez6Kp2GQP5/e15fm9XVIBqWsYuV5/I8Au4BgCzMoJzDG8PdvdxJax4/P7u9HHT+9fEQpK1myxy8izYBfAe9bsX3lXF9vOcqmQ5n8cVh7LfpKuQCrunr+CzwJlF5oBRGZIiLxIhKflpbmvGSqWr37UyKPzkmgS3gwt/RsbnUcpRQWFH4R+TVwwhiz8WLrGWNijTHRxpjosLAwJ6VT1emrhCO89P1uRnZtQtyUvnh76eBrSrkCK753DwBGichIwB8IFpFPjDF3WJBFOUhmbiF/+nI7PVvW4/XxkdTy1hPIlHIVTv80GmOeMcY0M8a0AsYDy7Tou5eSUsN/Fu8lO7+YF8d01aKvlIvRT6Sqdq8v2cvHa5KZ0Ls5HRvrSVtKuRpLT7EwxiwHlluZQVWv/KISZq5N5vpOjfjHmG5Wx1FKVUL3+FW1+nhNEhm5RUwa0Epn0lLKRelJ1apa5BeV8Ndvd/LZhsNc36kh/do0sDqSUuoCtPCrq5aSkct9MzeyM/U0t/dpweM3dNC9faVcmBZ+dcWKS0p588dEpq88gDHw/l3RDO2k4+sr5eq08KsrUlpqeOTTBBZsS+X6To14dmRHWofVsTqWUqoKtPCrKxK34RALtseR1q4AABAZSURBVKXy1IiO/H5QG6vjKKUug57Voy5bSakhdsUBIpuHcP91ra2Oo5S6TFr41WWbtS6Z5FO53H9dGz2Iq1QNpF09qsrSsgv4cNVB3l95kGvbhTK8ix7IVaom0sKvqmTf8Wwe/jSBXamnGdwhjH//NlL39pWqobTwq0uKT0rntvfW4eUFH07sxeAODa2OpJS6Clr41UUt33OCB2ZtIjzEn7n39ycsyM/qSEqpq6QHd9UFJabl8OCsTbRqUJvP7uunRV8pN6GFX1XKGMOzn2/D20v44O5eNAz2tzqSUqqaaOFXldqSksW6g+k8PqwDjetq0VfKnWjhV5X6YlMKvrW8GBPV1OooSqlqpoVfnee7bal8su4QI7s2Jtjfx+o4SqlqpoVfnWPtgVM8FLeZyOYh/F1n0FLKLWnhV2VKSw0vfLOTxsH+fDSpN3X89GxfpdyRFn5VZlXiSXalnubxYe216CvlxrTwK8B2+uYna5MJCfThV92bWB1HKeVATt+tE5HmwMdAY6AUiDXGvO7sHMpmW0oWqxJPsmz3CdYfTGfq4Lb41fK2OpZSyoGs+D5fDDxujNkkIkHARhFZbIzZaUEWj3U4PZcXvtnJkl3HAYgIrc0Lo7pwV7+WFidTSjma0wu/MSYVSLXfzhaRXUBTQAu/k5w4nc+UmRs5nJ7Lo9e3465+rahf29fqWEopJ7H0CJ6ItAJ6AOsqeWwKMAWgRYsWTs3lrvYdzyZ2xQG+TDhCUYnhtXHXMKZHM6tjKaWczLLCLyJ1gPnAo8aY0xUfN8bEArEA0dHRxsnx3E5OQTH3fhxP8qlcmtT1563bo+jRop7VsZRSFrCk8IuID7aiP8sY87kVGTyJMYan5m/lcHour4+PZEDbUELr6EibSnkqK87qEWA6sMsY8x9nb98TvffzARZsTeWpER25KVLH3lHK01mxxz8AuBPYJiIJ9mXPGmO+syCLWyouKSU9t5CvNh9lfVI6i3ceZ3iXRtwX09rqaEopF2DFWT0rAZ2s1UGMMUydvZmFO46VLZs4oBXPjuyEl5c2u1JKp150K/lFJTz/1Q4W7jhGVIsQnhjekZYNAgkPCbA6mlLKhWjhdyP/XbKPOfGHubNvS/7ym874eOuIHEqp82nhdwMFxSU88/k2Pt90hLFRzfjb6K5WR1JKuTAt/DWUMYZ1B9N5bfFeNh3KoKjE8PCQtjw0tJ3V0ZRSLk4Lfw311vJEXv1hDw2D/Jg0MIIBbUKJaR9mdSylVA2ghb8G2n4ki/8t28917cN4+44oAn3116iUqjqtGDVIYXEp329P5bHPtuAl8Oj17bToK6Uum1YNF3cqp4Dvtx9j/4kclu0+waH0XJrVC+CTe/rQKrS21fGUUjWQFn4XVVpq+Ou3O5mxOgmAAB9vOocH88CgNgzt1IiwIB1rRyl1ZbTwW2RjcgaLdhyjRYNAwusGENM+DG/7lbVFJaU8OW8rX2w+wpgeTbmlZzMGtA21OLFSyl1o4Xey/KISpn29g3kbUygu/WW06d9cE07X8GDi1h8iJNCXhMOZPDG8Aw8MaoNtXDullKoeWvid6J2fEnlj6T7yikq4o09L/ji8A0cy8vhw1UHmbkzhmy1HCfavxZHMPF66uRsTeusENEqp6ifGuP4cJ9HR0SY+Pt7qGFdk4fZUZq5N5sTpAvadyKFhkB+PD2vPuF7nFvW07AKy8gpp2aA2OfnF1NOpEJVSV0lENhpjoisu1z1+B9p0KIOH4jbTMMifjo2D+HX3cB4c3IZalYyhExbkV3bAVou+UsqRtPBXkxOn86kb6MOZghLqBviw/UgWU2dtonFdf76dei11A32sjqiUUoAHFf5Xf9jNmz8mcvClkdV+sDR2RSL/+G43QX61yC4oxtfbi8KSUuoF+jDzrj5a9JVSLsVjCv+bPyYCkJlbVK1dKcYYPll7iCC/WjStF0CrBrUJDwmgY5MghnVuREigdtsopVyLxxT+s45k5l114c/MLWTtgVPc0LkxXyUc4VB6Lv8Y043b+uhZOEop1+cRhb/8mUtHM/Po2rRulZ97Or8ILxHq+Nma6kxBMePeXcue49n4+3iRX1RK05AAhndpVO25lVLKEdy+8BeVlDL8tRVl949m5lXpeYt2HGNn6mmmrzxIvUBf/jm2O8v3niA1M589x7MZ0rEhy3afoEFtX5Y8dh0Bvt6OegtKKVWt3L7wJ508w4GTZ8ruH83Kv+j6+UUlfL7pCM9+sQ2Avq3rszUliwnvrS1bZ9Q14bwxoQdrEk/RNCRAi75SqkZx+8KfmGYr+k3q+lNQXMr+EzkYY847sycrt4gXv9tJwuFM9h7PoUFtX2be04dOTYL4ed9J/vTldv4xphvhIf5lk5f3a9PA6e9HKaWuliWzcYvICBHZIyL7ReRpR27rwMkcABb9IYbhXRqxbPcJhr22gvUH01m+5wRFJaXkF5XwwOyNfBafwt7jOXRqEsysyX3oHB6MiBDTPowVTw5mYLtQWofVwd9H9/CVUjWX0/f4RcQbeBO4AUgBNojI18aYnY7Y3sG0M4QF+RHk78PfR3cjsnkIT83fxm/fXXPeuvfFtCb5VC4v3dxNr55VSrktK7p6egP7jTEHAETkU+AmoNoL/3+X7GXuxhT6RNQHwNtLGNerBdNXHmTv8RxCAn3IzC3C19uLV2/tzk2RTas7glJKuRwrCn9T4HC5+ylAn4oricgUYApAixZXdn58eEgAN0WGM7rHuQX99fE9+GbLUR4Y3JZ3lidye98WNKkbcEXbUEqpmsbpo3OKyK3AcGPMvfb7dwK9jTEPXeg5NXl0TqWUssqFRue04uBuCtC83P1mwFELciillEeyovBvANqJSISI+ALjga8tyKGUUh7J6X38xphiEZkK/AB4Ax8YY3Y4O4dSSnkqSy7gMsZ8B3xnxbaVUsrTWXIBl1JKKeto4VdKKQ+jhV8ppTyMFn6llPIwTr+A60qISBqQfIVPDwVOVmOc6uKqucB1s2muy+OqucB1s7lbrpbGmLCKC2tE4b8aIhJf2ZVrVnPVXOC62TTX5XHVXOC62Twll3b1KKWUh9HCr5RSHsYTCn+s1QEuwFVzgetm01yXx1Vzgetm84hcbt/Hr5RS6lyesMevlFKqHC38SinlYdy68DtzUvcqZEkSkW0ikiAi8fZl9UVksYjss/+s54QcH4jICRHZXm5ZpTnE5g17+20VkSgLsk0TkSP2dksQkZHlHnvGnm2PiAx3UKbmIvKjiOwSkR0i8oh9ueVtdpFsVreZv4isF5Et9lwv2JdHiMg6e5vNsQ/Ljoj42e/vtz/eysm5ZojIwXLtFWlf7uy/f28R2Swi39rvO669jDFu+Q/bkM+JQGvAF9gCdLYwTxIQWmHZK8DT9ttPA/90Qo4YIArYfqkcwEjge0CAvsA6C7JNA/5Yybqd7b9TPyDC/rv2dkCmJkCU/XYQsNe+bcvb7CLZrG4zAerYb/sA6+xt8Rkw3r78HeD39tsPAO/Yb48H5jiovS6UawZwSyXrO/vv/zFgNvCt/b7D2sud9/jLJnU3xhQCZyd1dyU3AR/Zb38EjHb0Bo0xK4D0Kua4CfjY2KwFQkSkiZOzXchNwKfGmAJjzEFgP7bfeXVnSjXGbLLfzgZ2YZs32vI2u0i2C3FWmxljTI79ro/9nwGGAPPsyyu22dm2nAcMFRFxYq4LcdrvUkSaAb8C3rffFxzYXu5c+Cub1P1iHwpHM8AiEdkotonkARoZY1LB9iEGGlqU7UI5XKUNp9q/an9QrjvM6dnsX6l7YNtTdKk2q5ANLG4ze7dFAnACWIzt20WmMaa4km2X5bI/ngU0cEYuY8zZ9nrR3l6viYhfxVyVZK5u/wWeBErt9xvgwPZy58Jf2f+AVp67OsAYEwXcCDwoIjEWZqkqV2jDt4E2QCSQCvzbvtyp2USkDjAfeNQYc/piq1ayzKFtVkk2y9vMGFNijInENqd2b6DTRbZtWS4R6Qo8A3QEegH1gaecmUtEfg2cMMZsLL/4Itu+6lzuXPhdalJ3Y8xR+88TwBfYPgzHz351tP88YVG8C+WwvA2NMcftH9ZS4D1+6ZpwWjYR8cFWWGcZYz63L3aJNqssmyu02VnGmExgObY+8hAROTvrX/ltl+WyP16Xqnf5XW2uEfYuM2OMKQA+xPntNQAYJSJJ2Lqkh2D7BuCw9nLnwu8yk7qLSG0RCTp7GxgGbLfn+Z19td8BX1mR7yI5vgbusp/d0BfIOtu94SwV+lTHYGu3s9nG289wiADaAesdsH0BpgO7jDH/KfeQ5W12oWwu0GZhIhJivx0AXI/t+MOPwC321Sq22dm2vAVYZuxHLp2Qa3e5/8AFWz96+fZy+O/SGPOMMaaZMaYVtjq1zBhzO45sL0cepbb6H7aj8nux9S8+Z2GO1tjOptgC7DibBVu/3FJgn/1nfSdkicP29b8I257DPRfKge0r5Zv29tsGRFuQbaZ921vtf/BNyq3/nD3bHuBGB2UaiO1r9FYgwf5vpCu02UWyWd1m3YHN9u1vB/5S7nOwHttB5bmAn325v/3+fvvjrZ2ca5m9vbYDn/DLmT9O/fu3b3MQv5zV47D20iEblFLKw7hzV49SSqlKaOFXSikPo4VfKaU8jBZ+pZTyMFr4lVLKw2jhV25NRErKjbqYIJcYpVVE7heRu6phu0kiEnoFzxsuttE164nId1ebQ6nK1Lr0KkrVaHnGdol+lRhj3nFkmCq4FtuFOzHAKouzKDelhV95JPvl8XOAwfZFtxlj9ovINCDHGPMvEXkYuB8oBnYaY8aLSH3gA2wX1+QCU4wxW0WkAbYL0MKwXVQj5bZ1B/AwtuHB1wEPGGNKKuQZh23MmNbYRl9sBJwWkT7GmFGOaAPlubSrR7m7gApdPePKPXbaGNMb+B+2sVEqehroYYzpju0/AIAXgM32Zc8CH9uXPw+sNMb0wHa1bAsAEekEjMM2SF8kUALcXnFDxpg5/DIXQTdsV5H20KKvHEH3+JW7u1hXT1y5n69V8vhWYJaIfAl8aV82EBgLYIxZJiINRKQutq6Zm+3LF4hIhn39oUBPYIN9yPQALjwYXztswwMABBrbGPtKVTst/MqTmQvcPutX2Ar6KODPItKFiw+JW9lrCPCRMeaZiwUR23ScoUAtEdkJNLGPG/+QMebni78NpS6PdvUoTzau3M815R8QES+guTHmR2wTZIQAdYAV2LtqRGQQcNLYxsAvv/xG4OzkJ0uBW0Skof2x+iLSsmIQY0w0sABb//4r2Abyi9SirxxB9/iVuwuw7zmftdAYc/aUTj8RWYdtB2hChed5A5/Yu3EEeM0Yk2k/+PuhiGzFdnD37PC4LwBxIrIJ+Ak4BGCM2Skif8I2+5oXtpFHHwSSK8kahe0g8APAfyp5XKlqoaNzKo9kP6sn2hhz0uosSjmbdvUopZSH0T1+pZTyMLrHr5RSHkYLv1JKeRgt/Eop5WG08CullIfRwq+UUh7m/wE4aaRbnGfegAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(range(len(scores_window)), scores_window, label='Average training score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('nav_result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
